"""
Representation Learning - Intro (Lecture 1.1)

1. What is representation learning?
   In linear regression, features (like height, weight, age) are given by humans.
   Representation learning is about learning these features automatically from raw data,
   instead of relying on handcrafted ones.
   The learned features are called representations or embeddings.

2. Mapping nodes to d-dimensional embeddings
   In a graph (e.g., social network):
     - Nodes = entities (people)
     - Edges = relationships (friendships)
   We map each node to a vector of length d. For example:
       Alice   -> [0.9, 1.2, -0.3]
       Bob     -> [0.8, 1.0, -0.2]
       Charlie -> [-1.5, 0.4, 2.0]
   These vectors allow us to run standard ML algorithms that expect numeric input.

3. "Similar nodes should be close together"
   The idea is that if two nodes are similar (e.g., have similar neighbors or attributes),
   their vectors should be close in this embedding space.
   Close means small Euclidean distance or high cosine similarity.
   Example:
       Alice and Bob are friends with many of the same people → embeddings close.
       Charlie is from a different community → embedding far apart.

4. Why GNNs care about this
   Graph Neural Networks learn these embeddings automatically by:
       1. Using a node's own features (if available)
       2. Aggregating features from its neighbors
   This process repeats for several layers, making connected and similar nodes
   end up close in embedding space.

5. Relation to linear regression
   In linear regression: feature vector x is given.
   In GNNs: feature vector x is learned from graph structure and optional attributes.
   After learning these vectors, one can use simple models like linear/logistic regression
   for downstream tasks.

One-line intuition:
Representation learning = Let the model figure out the best way to describe each entity
as a vector, so that similar entities have similar coordinates.
"""


"""
Features, Euclidean Distance, and Cosine Similarity

1. Feature = input
   A feature is an input to the model, usually represented as numbers in a vector.
   Example in linear regression:
       Features for house price prediction might be:
       [size_in_sqft, number_of_rooms, distance_to_city]
   In GNNs, after representation learning, each node has a learned feature vector
   (embedding) that can be used as input to downstream models.

2. Euclidean distance
   Measures the straight-line distance between two points in space.
   Formula (for vectors a and b of dimension d):
       Euclidean(a, b) = sqrt( (a1 - b1)^2 + (a2 - b2)^2 + ... + (ad - bd)^2 )
   Example:
       a = [0, 0]
       b = [3, 4]
       distance = sqrt( (3-0)^2 + (4-0)^2 ) = sqrt(9 + 16) = sqrt(25) = 5
   A small Euclidean distance means points are close together.

3. Cosine similarity
   Measures the angle between two vectors, not their distance.
   Formula:
       cosine_similarity(a, b) = (a · b) / (||a|| * ||b||)
       where:
           a · b = dot product of a and b
           ||a|| = length (magnitude) of vector a
           ||b|| = length (magnitude) of vector b
   Range:
       +1   → vectors point in the exact same direction
        0   → vectors are at 90 degrees (no similarity)
       -1   → vectors point in opposite directions
   Example:
       a = [1, 0]
       b = [10, 0]
       cosine similarity = 1 (same direction, different magnitude)

4. Why these are used in GNNs
   After learning embeddings, similarity between two nodes can be measured by:
       - Euclidean distance: how far apart they are in space
       - Cosine similarity: how aligned their direction is in space
"""

